{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch torchvision torchaudio\n",
        "!pip install --quiet transformers datasets peft evaluate opacus\n"
      ],
      "metadata": {
        "id": "JVnT6r74l2Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import peft\n",
        "import accelerate\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "from peft import PromptTuningConfig, PrefixTuningConfig, LoraConfig, get_peft_model, TaskType\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ],
      "metadata": {
        "id": "AWYwXP7ImZzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"prajjwal1/bert-tiny\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "print(\"TinyBERT model and tokenizer loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "DHuFHtLRmyYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "g8fg_TlvslGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets_dict = {}\n",
        "\n",
        "print(\"Downloading SST2...\")\n",
        "datasets_dict[\"sst2\"] = load_dataset(\"glue\", \"sst2\")\n",
        "print(\"SST2 downloaded!\")\n",
        "\n",
        "print(\"Downloading QNLI...\")\n",
        "datasets_dict[\"qnli\"] = load_dataset(\"glue\", \"qnli\")\n",
        "print(\"QNLI downloaded!\")\n",
        "\n",
        "print(\"Downloading MNLI...\")\n",
        "datasets_dict[\"mnli\"] = load_dataset(\"glue\", \"mnli\")\n",
        "print(\"MNLI downloaded!\")\n",
        "\n",
        "print(\"Downloading QQP...\")\n",
        "datasets_dict[\"qqp\"] = load_dataset(\"glue\", \"qqp\")\n",
        "print(\"QQP downloaded!\")\n"
      ],
      "metadata": {
        "id": "rGhLgFpxm4si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task in [\"sst2\", \"qnli\", \"mnli\", \"qqp\"]:\n",
        "    print(f\"\\nSample from {task.upper()}:\\n\", datasets_dict[task]['train'][0])\n"
      ],
      "metadata": {
        "id": "An94843Ru8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function_sst2(example):\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "def tokenize_function_qnli(example):\n",
        "    return tokenizer(example[\"question\"], example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "def tokenize_function_mnli(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "def tokenize_function_qqp(example):\n",
        "    return tokenizer(example[\"question1\"], example[\"question2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenize_functions = {\n",
        "    \"sst2\": tokenize_function_sst2,\n",
        "    \"qnli\": tokenize_function_qnli,\n",
        "    \"mnli\": tokenize_function_mnli,\n",
        "    \"qqp\": tokenize_function_qqp,\n",
        "}\n"
      ],
      "metadata": {
        "id": "8cQZhSH2wyq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = {}\n",
        "for task in [\"sst2\", \"qnli\", \"mnli\", \"qqp\"]:\n",
        "    print(f\"Tokenizing {task}...\")\n",
        "    tokenized_datasets[task] = datasets_dict[task].map(tokenize_functions[task], batched=True)\n",
        "print(\"All datasets tokenized!\")\n"
      ],
      "metadata": {
        "id": "9xF1ZU8Jw3LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_info = {\n",
        "    \"sst2\": {\"num_labels\": 2, \"eval_split\": \"validation\"},\n",
        "    \"qnli\": {\"num_labels\": 2, \"eval_split\": \"validation\"},\n",
        "    \"qqp\": {\"num_labels\": 2, \"eval_split\": \"validation\"},\n",
        "    \"mnli\": {\"num_labels\": 3, \"eval_split\": \"validation_matched\"},\n",
        "}\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "noise_multiplier = 0.376"
      ],
      "metadata": {
        "id": "NS1kD3EMJIiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Functions"
      ],
      "metadata": {
        "id": "R2YzAs8VJp9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soft_prompt_model(num_labels):\n",
        "    prompt_config = PromptTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=20\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    return get_peft_model(base_model, prompt_config)\n",
        "\n",
        "def get_prefix_model(num_labels):\n",
        "    prefix_config = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=20\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    return get_peft_model(base_model, prefix_config)\n",
        "\n",
        "def get_lora_model(num_labels):\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"query\", \"value\"]\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    return get_peft_model(base_model, lora_config)\n",
        "\n",
        "def get_softprompt_lora_model(num_labels):\n",
        "    prompt_config = PromptTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=20\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    model = get_peft_model(base_model, prompt_config)\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"query\", \"value\"]\n",
        "    )\n",
        "    return get_peft_model(model, lora_config)\n",
        "\n",
        "def get_prefix_lora_model(num_labels):\n",
        "    prefix_config = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=20\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    model = get_peft_model(base_model, prefix_config)\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"query\", \"value\"]\n",
        "    )\n",
        "    return get_peft_model(model, lora_config)\n",
        "\n",
        "def get_full_finetune_model(num_labels):\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "\n",
        "def get_last_layer_finetune_model(num_labels):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"prajjwal1/bert-tiny\", num_labels=num_labels\n",
        "    )\n",
        "    for name, param in model.named_parameters():\n",
        "        if not name.startswith(\"classifier\"):\n",
        "            param.requires_grad = False\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "naNv5pVCsjfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training function without DP (Epsilon = infinity)"
      ],
      "metadata": {
        "id": "kWR7DNhbKGfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, num_epochs=3, lr=2e-5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            batch[\"labels\"] = batch.pop(\"label\")\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_VVyve4xMIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Function With DP(Epsilon = (8)"
      ],
      "metadata": {
        "id": "uMW3829dLBfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_dp(model, train_dataloader, num_epochs=num_epochs, lr=learning_rate):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    model.train()\n",
        "    privacy_engine = PrivacyEngine()\n",
        "    model, optimizer, train_dataloader = privacy_engine.make_private(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=train_dataloader,\n",
        "        noise_multiplier=noise_multiplier,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "    )\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            batch[\"labels\"] = batch.pop(\"label\")\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    print(f\"(ε, δ)-DP with ε = {epsilon:.2f}, δ = 1e-5\")\n",
        "    return epsilon"
      ],
      "metadata": {
        "id": "3lHn_mCuLHun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate Function"
      ],
      "metadata": {
        "id": "6E4Ns_EfK2_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, eval_dataloader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        batch[\"labels\"] = batch.pop(\"label\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    final_score = metric.compute()\n",
        "    return final_score[\"accuracy\"]"
      ],
      "metadata": {
        "id": "1JK8g2GoK2GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Loop for all methods and datasets (without DP when epsilon =infinity)"
      ],
      "metadata": {
        "id": "r-hu4AYjLgPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = {\n",
        "    \"soft_prompt\": get_soft_prompt_model,\n",
        "    \"prefix\": get_prefix_model,\n",
        "    \"lora\": get_lora_model,\n",
        "    \"full_finetuning\": get_full_finetune_model,\n",
        "    \"last_layer_finetuning\": get_last_layer_finetune_model,\n",
        "    \"soft_prompt_lora\": get_softprompt_lora_model,\n",
        "    \"prefix_lora\": get_prefix_lora_model\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for method_name, model_func in methods.items():\n",
        "    print(f\"\\n=== Running {method_name.replace('_', ' ').title()} (No DP) ===\")\n",
        "    results[method_name] = {}\n",
        "    for task, info in dataset_info.items():\n",
        "        print(f\"\\n--- Dataset: {task.upper()} ---\")\n",
        "        tokenized_datasets[task].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "        train_dataloader = DataLoader(tokenized_datasets[task][\"train\"], batch_size=batch_size, shuffle=True)\n",
        "        eval_dataloader = DataLoader(tokenized_datasets[task][info[\"eval_split\"]], batch_size=batch_size)\n",
        "        model = model_func(info[\"num_labels\"])\n",
        "        model.print_trainable_parameters()\n",
        "        # Adjust learning rate and epochs for each method if needed\n",
        "        if method_name in [\"soft_prompt\", \"prefix\", \"lora\", \"soft_prompt_lora\", \"prefix_lora\"]:\n",
        "            train_model(model, train_dataloader, num_epochs=10, lr=0.3)\n",
        "        elif method_name == \"last_layer_finetuning\":\n",
        "            train_model(model, train_dataloader, num_epochs=10, lr=0.01)\n",
        "        else:\n",
        "            train_model(model, train_dataloader, num_epochs=3, lr=2e-5)\n",
        "        accuracy = evaluate_model(model, eval_dataloader)\n",
        "        print(f\"{task.upper()} {method_name.replace('_', ' ').title()} Validation Accuracy: {accuracy:.4f}\")\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        results[method_name][task] = {\n",
        "            \"trainable_params\": trainable_params,\n",
        "            \"accuracy\": accuracy\n",
        "        }\n",
        "print(\"\\n=== Results (No DP, ε = ∞) ===\")\n",
        "for method, res in results.items():\n",
        "    print(f\"\\n{method.replace('_', ' ').title()}:\")\n",
        "    for task, vals in res.items():\n",
        "        print(f\"  {task.upper()}: Params={vals['trainable_params']}, Accuracy={vals['accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ABXJL7K6smhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main loop for all methods and datasets(with DP when Epsilon =8)"
      ],
      "metadata": {
        "id": "e9IV0PU2MaYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_all_methods_dp(tokenized_datasets):\n",
        "    methods = {\n",
        "        \"soft_prompt\": get_soft_prompt_model,\n",
        "        \"prefix\": get_prefix_model,\n",
        "        \"lora\": get_lora_model,\n",
        "        \"full_finetuning\": get_full_finetune_model,\n",
        "        \"last_layer_finetuning\": get_last_layer_finetune_model,\n",
        "        \"soft_prompt_lora\": get_softprompt_lora_model,\n",
        "        \"prefix_lora\": get_prefix_lora_model\n",
        "    }\n",
        "    results = {}\n",
        "    for method_name, model_func in methods.items():\n",
        "        print(f\"\\n=== Running {method_name.replace('_', ' ').title()} with DP ===\")\n",
        "        results[method_name] = {}\n",
        "        for task, info in dataset_info.items():\n",
        "            print(f\"\\n--- Dataset: {task.upper()} ---\")\n",
        "            # Strictly set format to only tensors for required columns\n",
        "            tokenized_datasets[task].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "            train_dataloader = DataLoader(tokenized_datasets[task][\"train\"], batch_size=batch_size, shuffle=True)\n",
        "            eval_dataloader = DataLoader(tokenized_datasets[task][info[\"eval_split\"]], batch_size=batch_size)\n",
        "            # Debug: check batch types\n",
        "            batch = next(iter(train_dataloader))\n",
        "            print(\"Batch types:\", {k: type(v) for k, v in batch.items()})\n",
        "            model = model_func(info[\"num_labels\"])\n",
        "            #model.print_trainable_parameters()\n",
        "            epsilon = train_model_dp(model, train_dataloader)\n",
        "            accuracy = evaluate_model(model, eval_dataloader)\n",
        "            print(f\"{task.upper()} {method_name.replace('_', ' ').title()} Validation Accuracy: {accuracy:.4f}\")\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(f\"Trainable params: {trainable_params}\")\n",
        "\n",
        "            results[method_name][task] = {\n",
        "                \"trainable_params\": trainable_params,\n",
        "                \"epsilon\": epsilon,\n",
        "                \"accuracy\": accuracy\n",
        "            }\n",
        "    return results\n",
        "results = run_all_methods_dp(tokenized_datasets)\n"
      ],
      "metadata": {
        "id": "-MvpD_nPMatU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}